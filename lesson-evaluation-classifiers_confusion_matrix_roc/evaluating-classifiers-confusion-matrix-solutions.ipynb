{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Evaluating Classifiers: The Confusion Matrix and AUC-ROC\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Joseph Nelson (DC)_\n",
    "\n",
    "_Modified for DSI-EAST by Justin Pounders(ATL)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Define true positives, false positives, true negatives, and false negatives\n",
    "- Create and interpret a confusion matrix\n",
    "- Calculate the common classification evaluation metrics\n",
    "- Describe how the threshold affects predictions in a classification model\n",
    "- Define and plot the receiver operating characteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: evaluating classifiers](#intro)\n",
    "- [The baseline accuracy](#baseline)\n",
    "- [Build a KNN model to predict spam](#knn)\n",
    "- [Predicting labels vs. predicting probabilities](#labels-vs-probs)\n",
    "- [The confusion matrix](#confusion-matrix)\n",
    "- [Review: Type I error and p-values](#type1-pvalues)\n",
    "- [Type II error and power](#type2-power)\n",
    "- [Fundamental classifier metrics](#metrics)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Sensitivity / Recall / True Positive Rate (TPR)](#sensitivity)\n",
    "    - [False Positive Rate (FPR)](#fpr)\n",
    "    - [Specificity / True Negative Rate (TNR)](#specificity)\n",
    "    - [Precision / Positive Predictive Value](#precision)\n",
    "- [The F1-score and sklearn's `classification_report`](#f1-score)\n",
    "- [Changing the threshold for prediction](#threshold)\n",
    "    - [Load the UCI breast cancer data](#uci)\n",
    "    - [Evaluate prediction on a test set](#testing)\n",
    "- [The ROC curve](#roc-curve)\n",
    "- [Reference table of common classification metric terms and definitions](#table)\n",
    "- [Additional resources](#resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: evaluating classifiers\n",
    "\n",
    "---\n",
    "\n",
    "The evaluation of classifier models is not as straightforward as regression models. In this lesson we will cover the fundamentals of evaluating classifiers.\n",
    "\n",
    "To ground the theory in a real world example, we will use a spam dataset to build and evaluate classifiers on. The spam dataset is 1001 columns wide, containing an `is_spam` binary marker for whether a given email was spam or not, and then 1000 columns, each correpsonding to a word that could have appeared in the email (also marked with 0 or 1).\n",
    "\n",
    "**Load the spam data below and print out the header.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv('./datasets/spam_words_wide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>getzed</th>\n",
       "      <th>86021</th>\n",
       "      <th>babies</th>\n",
       "      <th>sunoco</th>\n",
       "      <th>ultimately</th>\n",
       "      <th>thk</th>\n",
       "      <th>voted</th>\n",
       "      <th>spatula</th>\n",
       "      <th>fiend</th>\n",
       "      <th>...</th>\n",
       "      <th>itna</th>\n",
       "      <th>borin</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>iccha</th>\n",
       "      <th>videochat</th>\n",
       "      <th>freefone</th>\n",
       "      <th>pist</th>\n",
       "      <th>reformat</th>\n",
       "      <th>strict</th>\n",
       "      <th>69698</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_spam  getzed  86021  babies  sunoco  ultimately  thk  voted  spatula  \\\n",
       "0        0       0      0       0       0           0    0      0        0   \n",
       "1        0       0      0       0       0           0    0      0        0   \n",
       "2        1       0      0       0       0           0    0      0        0   \n",
       "3        0       0      0       0       0           0    0      0        0   \n",
       "4        0       0      0       0       0           0    0      0        0   \n",
       "\n",
       "   fiend  ...    itna  borin  thoughts  iccha  videochat  freefone  pist  \\\n",
       "0      0  ...       0      0         0      0          0         0     0   \n",
       "1      0  ...       0      0         0      0          0         0     0   \n",
       "2      0  ...       0      0         0      0          0         0     0   \n",
       "3      0  ...       0      0         0      0          0         0     0   \n",
       "4      0  ...       0      0         0      0          0         0     0   \n",
       "\n",
       "   reformat  strict  69698  \n",
       "0         0       0      0  \n",
       "1         0       0      0  \n",
       "2         0       0      0  \n",
       "3         0       0      0  \n",
       "4         0       0      0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 1001) 0.13406317300789664\n"
     ]
    }
   ],
   "source": [
    "print(spam.shape, spam.is_spam.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "\n",
    "## The baseline accuracy\n",
    "\n",
    "---\n",
    "\n",
    "The importance of calculating your baseline accuracy when building classifiers cannot be overstated. It is critical to know the baseline when you are evaluating a classifier using accuracy.\n",
    "\n",
    "> **Baseline Accuracy**: The accuracy that can be achieved by a model by simply guessing the majority class for every observation.\n",
    "\n",
    "As human beings we are inclined to think that \"50% accuracy\" is equivalent to guessing by chance. In fact, a 50% accuracy only equates to guessing by chance in a very specific context: when we have equal proportion of positive and negatvie (1 and 0) target class labels in our dataset, or in the multi-class case when the majority class makes up 50% of the labels.\n",
    "\n",
    "> **`baseline_accuracy = majority_class_N / total_N`**\n",
    "\n",
    "In a binary class problem the reality is that your dataset is more likely to be unbalanced, and the more unbalanced it is the higher the baseline accuracy becomes. This is important to remember because if 99% of your observations are of one class, predicting 99% of them correctly with a model is performing at chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the baseline accuracy for the spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_acc = 1 - spam['is_spam'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc # class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## Set up a KNN model to predict spam\n",
    "\n",
    "---\n",
    "\n",
    "We will use just the first 250 columns for the sake of speed in fitting and prediction. Even with a fourth of the predictors the cross-validation can be noticeably slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y = spam.is_spam.values\n",
    "X = spam.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.91935484  0.93369176  0.9265233   0.9265233   0.93010753  0.92100539\n",
      "  0.92459605  0.9352518   0.94064748  0.9442446 ]\n",
      "0.9301946038\n"
     ]
    }
   ],
   "source": [
    "accs = cross_val_score(knn, X, y, cv=10)\n",
    "print(accs)\n",
    "print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y = spam['is_spam'].values\n",
    "x = spam.iloc[:, 1:251]\n",
    "\n",
    "knn  = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validate the accuracy of the model\n",
    "\n",
    "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.89784946  0.88351254  0.88888889  0.87634409  0.89605735  0.88330341\n",
      "  0.89228007  0.89208633  0.90827338  0.91366906]\n",
      "0.893226458967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accs = cross_val_score(knn, x, y, cv=10)\n",
    "print(accs)\n",
    "print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='labels-vs-probs'></a>\n",
    "\n",
    "## Predicting labels vs. predicting probabilities\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn classification models come with two distinct prediction functions:\n",
    "1. `.predict()`: predicts the labels (classes) of observations \n",
    "2. `.predict_proba()` predict the *probability of membership to each class*.\n",
    "\n",
    "The `.predict()` function will return the predicted labels for a design matrix as a vector of integer labels. \n",
    "\n",
    "In contrast, the `.predict_proba()` function will return the probabilities as a matrix, where the columns are ordered in increasing order of the class labels (eg. the first column is probabilities for class 0, the second column is probabilities for class 1).\n",
    "\n",
    "**Fit the KNN model and print out the predicted labels and predicted probabilities for a few points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(X.iloc[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ],\n",
       "       [ 0.8,  0.2],\n",
       "       [ 0.8,  0.2],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0.8,  0.2],\n",
       "       [ 0.6,  0.4],\n",
       "       [ 0.8,  0.2],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0.2,  0.8],\n",
       "       [ 0. ,  1. ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X.iloc[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='confusion-matrix'></a>\n",
    "\n",
    "## The confusion matrix\n",
    "\n",
    "---\n",
    "\n",
    "The confusion matrix is a table representing the performance of your model to classify labels correctly.\n",
    "\n",
    "**A confusion matrix for a binary classification task:**\n",
    "\n",
    "|   |Predicted Negative | Predicted Positive |   \n",
    "|---|---|---|\n",
    "|**Actual Negative**  | True Negative (TN)  | False Positive (FP)  |\n",
    "|**Actual Positive** | False Negative (FN)  | True Positive (TP)  |\n",
    "\n",
    "In a binary classifier, the \"true\" class is typically labeled with 1 and the \"false\" class is labeled with 0. \n",
    "\n",
    "> **True Positive**: A positive class observation (1) is correctly classified as positive by the model.\n",
    "\n",
    "> **False Positive**: A negative class observation (0) is incorrectly classified as positive.\n",
    "\n",
    "> **True Negative**: A negative class observation is correctly classified as negative.\n",
    "\n",
    "> **False Negative**: A positive class observation is incorrectly classified as negative.\n",
    "\n",
    "Columns of the confusion matrix sum to the predictions by class. Rows of the matrix sum to the actual values within each class. You may encounter confusion matrices where the actual is in columns and the predicted is in the rows: the meaning is the same but the table will be reoriented.\n",
    "\n",
    "> **Note:** Remembering what the cells in the confusion matrix represents can be a little tricky. The first word (True or False) indicates whether or not the model was correct. The second word (Positive or Negative) indicates the *model's guess* (not the actual label!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the confusion matrix metrics for your model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp = np.sum((y == 1)) & (predicted == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = np.sum((y == 0) & (predicted == 0))\n",
    "fn = np.sum((y == 1) & (predicted == 0))\n",
    "fp = np.sum((y == 0) & (predicted == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4810\n",
      "314\n",
      "[0 0 0 ..., 0 0 0]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(tn)\n",
    "print(fn)\n",
    "print(tp)\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify this is the same as the numbers you get from sklearn's `metrics.confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4810,   15],\n",
       "       [ 314,  433]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, predicted) # orde of numbers follows the gird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='type1-pvalues'></a>\n",
    "\n",
    "## Review: Type I error and p-values\n",
    "\n",
    "---\n",
    "\n",
    "In the context of hypothesis testing false positives and false negatives are referred to as Type I and Type II error, respectively. \n",
    "\n",
    "Type I error is the incorrect rejection of the null hypothesis when in fact the null hypothesis is true. This is equivalent to the false positive rate in classification: the rate of a model labeling an observation as \"true\" when in fact it is \"false\". \n",
    "\n",
    "Type I error directly corresponds to p-values: **the p-value is the probability of incorrectly rejecting the null hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "\n",
    "## The fundamental classifier metrics\n",
    "\n",
    "---\n",
    "\n",
    "All metrics we use to evaluate classifiers are tied to the content of the confusion matrix. Remember that for the binary classification case we have four cells in the confusion matrix:\n",
    "\n",
    "- **`tp`**: true positives (classifier correct; classifier guessed 1)\n",
    "- **`fp`**: fasle positives (classifier incorrect; classifier guessed 1)\n",
    "- **`tn`**: true negative (classifier correct; classifier guessed 0)\n",
    "- **`fn`**: false negative (classifier incorrect; classifier guessed 0)\n",
    "\n",
    "Below are the fundamental metrics that data scientists use to evaluate the performance of their classifier model.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='accuracy'></a>\n",
    "### Accuracy\n",
    "\n",
    "With the total population as:\n",
    "\n",
    "> **`total_population = tp + fp + tn + fn`**\n",
    "\n",
    "The accuracy can be calculated as:\n",
    "\n",
    "> **`accuracy = (tp + tn) / total_population`**\n",
    "\n",
    "Which is just the proportion of correct guesses, regardless of class. The `.score()` function attached to sklearn classification model objects defaults to returning the accuracy of the model's predictions given an `X` and `y`.\n",
    "\n",
    "The inverse of the accuracy is known as the **misclassification rate**, which is calculated:\n",
    "\n",
    "> **`misclassification_rate = (fp + fn) / total_population`**\n",
    "\n",
    "**Calculate the accuracy using the confusion matrix cells.**\n",
    "- Validate that it is the same as `metrics.accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94095477386934678"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='sensitivity'></a>\n",
    "### Sensitivity / Recall / True Positive Rate\n",
    "\n",
    "The true positive rate is the percent of times that when the label was in fact 1 the model predicted 1. This is alternatively known as the **Sensitivity** or **Recall**. \n",
    "\n",
    "This is calculated as:\n",
    "\n",
    "> **`recall = tp / (tp + fn)`**\n",
    "\n",
    "**Calculate the recall with the confusion matrix cells.**\n",
    "- Validate that this is the same as `metrics.recall_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579651941098\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(y, predicted))\n",
    "print(tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='fpr'></a>\n",
    "### False Positive Rate\n",
    "\n",
    "Alternatively, the false positive rate measures the percent of times the model predicts a 1 when the target class is actually a 0. \n",
    "\n",
    "> **`fpr = fp / (tn + fp)`**\n",
    "\n",
    "**Calculate the FPR using the confusion matrix cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='specificity'></a>\n",
    "### Specificity / True Negative Rate\n",
    "\n",
    "The true negative rate measures the percent of times the classifier predicted the class was 0 out the number of actual times the class was 0. It is the sister metric to Sensitivity, which measures the same thing but for positives.\n",
    "\n",
    "> **`specificity = tn / (tn + fp)`**\n",
    "\n",
    "**Calculate the specificity using the confusion matrix cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='precision'></a>\n",
    "### Precision / Positive Predictive Value\n",
    "\n",
    "The precision measures the percent of times that the classifier was correct when it was predicting the true (1) class.\n",
    "\n",
    "> **`precision = tp / (tp + fp)`**\n",
    "\n",
    "The idea of the classifier being _precise_ is subtly different than it being _accurate_. Precision is a measure of correctness only for its positive class predictions, whereas accuracy is a measure of correctness for all guesses.\n",
    "\n",
    "**Calculate the precision using the confusion matrix cells.**\n",
    "- Validate that this is the same as `metrics.precision_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9665178571428571"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f1-score'></a>\n",
    "\n",
    "## F1-score and the `classification_report`\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn's `metrics.classification_report` helps diagnose the effectiveness of your classifier. The report focuses on the precision, recall, and a metric known as the f1-score.\n",
    "\n",
    "The f1-score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall metrics. Blending the two is useful: precision measures how effectively the classifier performs when it is predicting a 1, whereas recall measures how many of the total 1 classes out of all the 1-labeled observations were predicted correctly. \n",
    "\n",
    "### $$ F_1 = 2 \\cdot \\frac{1}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "By combining the two we have a measure of the classifiers ability to find the positive labeled observations as well as how permissive it is of identification errors on those labels.\n",
    "\n",
    "**A brief overview/guide:**\n",
    "- `Precision = True Positives / (True Positives + False Positives)`\n",
    "    - A precision score of 1 indicates that the classifier never mistakenly added observations from another class. A precision score of 0 would mean that the classifier misclassified every instance of the current class.\n",
    "- `Recall = True Positives / (True Positives + False Negatives)`\n",
    "    - A recall score of 1 indicates that the classifier correctly predicted (found) all observations of the current class (by implication, no false negatives, or misclassifications of the current class). A recall score of 0 alternatively means that the classifier missed all observations of the current class.\n",
    "- `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "    - The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once.\n",
    "- `Support` is simply the number of observations of the labelled class.\n",
    "\n",
    "You can print out the report of these three metrics on both of the classes (or more if you have a multi-class problem) using the `classification_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4825\n",
      "          1       0.97      0.58      0.72       747\n",
      "\n",
      "avg / total       0.94      0.94      0.93      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='threshold'></a>\n",
    "\n",
    "## Changing the threshold for prediction\n",
    "\n",
    "---\n",
    "\n",
    "The prediction of the classifier defaults to guessing the class that has the highest predicted probability. This neccessarily leads to the highest possible accuracy (**only a guarantee for the training data!**). \n",
    "\n",
    "However, it could be the case that maximizing the accuracy is not, in fact, our ultimate goal. Consider the following scenario:\n",
    "\n",
    "> **Cancer detection:** You have developed a classifier to detect, based on some medical measurements, whether or not a person has a cancerous tumor or not. Your classifier gets a 96% accuracy compared to a 60% baseline accuracy.\n",
    "\n",
    "Your classifier is performing well, but what might be wrong with just maximizing the accuracy in this case? Think back to the confusion matrix and your goal (to treat cancer patients before its too late)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='uci-data'></a>\n",
    "### Load the UCI breast cancer data\n",
    "\n",
    "Below we will load the medical data on breast cancer detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = ['id',\n",
    "                'clump_thickness',\n",
    "                'cell_size_uniformity',\n",
    "                'cell_shape_uniformity',\n",
    "                'marginal_adhesion',\n",
    "                'single_epithelial_size',\n",
    "                'bare_nuclei',\n",
    "                'bland_chromatin',\n",
    "                'normal_nucleoli',\n",
    "                'mitoses',\n",
    "                'class']\n",
    "\n",
    "bcw = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', \n",
    "                  names=column_names)\n",
    "\n",
    "bcw['bare_nuclei'] = bcw.bare_nuclei.map(lambda x: int(x) if not x == '?' else np.nan)\n",
    "bcw.dropna(inplace=True)\n",
    "\n",
    "y = bcw['class'].map(lambda x: 1 if x == 4 else 0)\n",
    "X = bcw.iloc[:, 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=25, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33)\n",
    "knn = KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the accuracy on the test set and compare to baseline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883632408918 0.859706362153\n"
     ]
    }
   ],
   "source": [
    "baseline = 1 - np.mean(y_test)\n",
    "print(knn.score(X_test, y_test), baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the predicted labels and predicted probabilities on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92,  0.08],\n",
       "       [ 0.6 ,  0.4 ],\n",
       "       [ 0.96,  0.04],\n",
       "       ..., \n",
       "       [ 0.96,  0.04],\n",
       "       [ 1.  ,  0.  ],\n",
       "       [ 1.  ,  0.  ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pp = knn.predict_proba(X_test)\n",
    "y_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the confusion matrix for your classfier's performance on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conmat = confusion_matrix(y_test, y_pred)\n",
    "confusion = pd.DataFrame(conmat, \n",
    "                         index=['is_healthy', 'is_cancer'],\n",
    "                         columns=['predicted_healthy', 'predicted_cancer'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='change-threshold'></a>\n",
    "\n",
    "### Lower the threshold for predicting cancer\n",
    "\n",
    "Right now the classifier is choosing to label cancer vs. healthy based on the 0.5 predicted probability threshold. \n",
    "\n",
    "Say our goal was to have 0 false negatives: in other words, in no case do we want to predict the person is healthy when in fact they have cancer!\n",
    "\n",
    "1. Create a dataframe of the predicted probabilities (class 0 and class 1 probabilities).\n",
    "2. Create a new column with predicted labels where the threshold for labeling cancer/1 is 10% rather than 50%\n",
    "    - In other words, the predicted probability of class 1 only needs to be greater than 0.10 for the label to be 1.\n",
    "3. Recreate the confusion matrix with the predictions using the new threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pp = pd.DataFrame(knn.predict_proba(X_test),\n",
    "                    columns=['class_0_pp','class_1_pp'])\n",
    "y_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pp['pred_class_thresh10'] = [1 if x>= 0.1 else 0 for x in y_pp.class_1_pp.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conmat = confusion_matrix(y_test, y_pp.pred_class_thresh10)\n",
    "confusion = pd.DataFrame(conmat,\n",
    "                         index=['is_healthy', 'is_cancer'],\n",
    "                         columns=['predicted_healthy', 'predicted_cancer'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='roc-curve'></a>\n",
    "\n",
    "## The Receiver operating characteristic (ROC) curve\n",
    "\n",
    "---\n",
    "\n",
    "The ROC curve is a popular visual of the performance of a classifier. It has a few attractive properties:\n",
    "- It compares the true positive rate to the false positive rate **as the threshold for predicting 1 changes**.\n",
    "- When the area under the curve is 0.50, this is equivalent to the baseline (chance) prediction.\n",
    "- When the area under the curve is 1.00, this is equivalent to perfect prediction.\n",
    "\n",
    "The area under the ROC curve is inherently related to the accuracy, but the AUC-ROC is preferred because it is automatically adjusted to the baseline and gives a robust picture of how the classifier performs at different threshold choices. \n",
    "\n",
    "**Note:**\n",
    "- As the class assignment threshold increases for the positive class (has cancer), the false positive rate and true positive rate necessarily increase.\n",
    "- For a classifier performing at chance, this is the diagonal dotted line: an equal chance of false positives and true positives.\n",
    "- The greater the area under the curve, the higher the ratio of true positives to false positives as the threshold becomes more lenient.\n",
    "- The greater the area under the curve, the higher the quality of the classification model. \n",
    "- Most classification problems will never get close to a full 1.0 area under the curve: the wisconsin breast cancer data is unique in that the signal is extremely strong. If you see this, it usually indicates that something is wrong with your procedure (or, if you are predicting on the training set, that your model is overfitting.)\n",
    "\n",
    "**Below is code to plot the ROC curve for our cancer data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc # auc= a read under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh= roc_curve(y_test, y_pp.class_1_pp)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for cancer detection', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i for i in zip(tpr, fpr, thresh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reference table of common classification metric terms and definitions\n",
    "\n",
    "<br><br>\n",
    "\n",
    "|  TERM | DESCRIPTION  |\n",
    "|---|---|\n",
    "|**TRUE POSITIVES** | The number of \"true\" classes correctly predicted to be true by the model. <br><br> `TP = Sum of observations predicted to be 1 that are actually 1`<br><br>The true class in a binary classifier is labeled with 1.|\n",
    "|**TRUE NEGATIVES** | The number of \"false\" classes correctly predicted to be false by the model. <br><br> `TP = Sum of observations predicted to be 0 that are actually 0`<br><br>The false class in a binary classifier is labeled with 0.|\n",
    "|**FALSE POSITIVES** | The number of \"false\" classes incorrectly predicted to be true by the model. This is the measure of **Type I error**.<br><br> `TP = Sum of observations predicted to be 1 that are actually 0`<br><br>Remember that the \"true\" and \"false\" refer to the veracity of your guess, and the \"positive\" and \"negative\" component refer to the guessed label.|\n",
    "|**FALSE NEGATIVES** | The number of \"true\" classes incorrectly predicted to be false by the model. This is the measure of **Type II error.**<br><br> `TP = Sum of observations predicted to be 0 that are actually 1`<br><br>|\n",
    "|**TOTAL POPULATION** | In the context of the confusion matrix, the sum of the cells. <br><br> `total population = tp + tn + fp + fn`<br><br>|\n",
    "|**SUPPORT** | The marginal sum of rows in the confusion matrix, or in other words the total number of observations belonging to a class regardless of prediction. <br><br>|\n",
    "|**ACCURACY** | The number of correct predictions by the model out of the total number of observations. <br><br> `accuracy = (tp + tn) / total_population`<br><br>|\n",
    "|**PRECISION** | The ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**RECALL/SENSITIVITY**    | The ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**SPECIFICITY** | Percent of times the classifier predicted 0 out of all the times the class was 0.<br><br> `specificity = tn / (tn + fp)`<br><br>|\n",
    "|**FALSE POSITIVE RATE** | Percent of times model predicts 1 when the class is 0.<br><br> `fpr = fp / (tn + fp)`<br><br>|\n",
    "|**F1-SCORE** | The harmonic mean of the precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates. <br><br>`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)` <br><br>_The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once._ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- An introduction to [Confusion Matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
    "- A deeper [Introduction to ROC](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf)\n",
    "- Receiver Operation Characteristic curves, [university resource](http://ebp.uga.edu/courses/Chapter%204%20-%20Diagnosis%20I/8%20-%20ROC%20curves.html)\n",
    "- Interactive [playing with ROC curves](http://www.navan.name/roc/)\n",
    "- Data School's video and transcript on [ROC/AUC](http://www.dataschool.io/roc-curves-and-auc-explained/)\n",
    "- Watch Rahul Patwari's [video](https://www.youtube.com/watch?v=21Igj5Pr6u4) on ROC curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
